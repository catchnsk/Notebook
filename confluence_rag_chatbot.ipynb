{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Confluence RAG Chatbot\n",
    "### End-to-End Conversational AI with Retrieval-Augmented Generation\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "This notebook builds a **production-ready conversational chatbot** that answers user questions about Confluence page data using **Retrieval-Augmented Generation (RAG)**.\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Confluence Pages â†’ LangChain Loader â†’ Text Splitter â†’ OpenAI Embeddings â†’ ChromaDB\n",
    "                                                                              â†“\n",
    "User Query â†’ Retriever (top_k) â†’ Prompt Template (strict grounding) â†’ GPT-3.5 Turbo â†’ Answer + Citations\n",
    "                                                                              â†“\n",
    "                                                              Gradio Chat UI (memory, clear, sources)\n",
    "```\n",
    "\n",
    "### Tech Stack\n",
    "| Component | Technology |\n",
    "|---|---|\n",
    "| Document Loading | `langchain_community.document_loaders.ConfluenceLoader` |\n",
    "| Text Splitting | `RecursiveCharacterTextSplitter` |\n",
    "| Embeddings | `OpenAIEmbeddings` (text-embedding-ada-002) |\n",
    "| Vector Store | `ChromaDB` (in-memory) |\n",
    "| LLM | `GPT-3.5 Turbo` (temperature 0.1) |\n",
    "| Chain Pattern | LCEL / Runnable chains (modern LangChain) |\n",
    "| UI | Gradio Blocks with chat interface |\n",
    "\n",
    "### Key Features\n",
    "- Strict grounded answering â€” only answers from retrieved context\n",
    "- Citations with page titles and source metadata\n",
    "- Conversational memory per session\n",
    "- Adjustable `top_k` retrieval via slider\n",
    "- Clear Chat button\n",
    "- Robust error handling throughout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How to Run Locally\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.9+\n",
    "- An OpenAI API key\n",
    "- Confluence instance URL + credentials (API token or personal access token)\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Clone or download** this notebook to your local machine.\n",
    "\n",
    "2. **Create a virtual environment** (recommended):\n",
    "   ```bash\n",
    "   python -m venv venv\n",
    "   source venv/bin/activate  # macOS/Linux\n",
    "   venv\\Scripts\\activate     # Windows\n",
    "   ```\n",
    "\n",
    "3. **Install dependencies** â€” Run the first code cell (Section 2) which installs all packages.\n",
    "\n",
    "4. **Set environment variables** for local use (instead of Colab userdata):\n",
    "   ```bash\n",
    "   export OPENAI_API_KEY=\"sk-your-key-here\"\n",
    "   export CONFLUENCE_URL=\"https://your-domain.atlassian.net/wiki\"\n",
    "   export CONFLUENCE_USERNAME=\"your-email@company.com\"\n",
    "   export CONFLUENCE_API_KEY=\"your-confluence-api-token\"\n",
    "   export CONFLUENCE_SPACE_KEY=\"YOUR_SPACE_KEY\"\n",
    "   ```\n",
    "   Or create a `.env` file in the same directory with these values.\n",
    "\n",
    "5. **Run all cells** sequentially in Jupyter Lab / Notebook:\n",
    "   ```bash\n",
    "   jupyter notebook confluence_rag_chatbot.ipynb\n",
    "   ```\n",
    "\n",
    "6. **Access the Gradio UI** â€” The last cell launches a Gradio interface. Open the local URL printed in the output (typically `http://127.0.0.1:7860`).\n",
    "\n",
    "> **Note for Colab users**: The notebook auto-detects Colab and uses `google.colab.userdata` for secrets. Set your secrets in Colab's sidebar (ðŸ”‘ icon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Setup: Install Dependencies & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2a. Install all required packages\n",
    "# ============================================================\n",
    "# Run this cell once â€” it installs everything needed.\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    \"\"\"Install a package using pip, suppressing verbose output.\"\"\"\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "\n",
    "packages = [\n",
    "    \"langchain>=0.3.0\",\n",
    "    \"langchain-community>=0.3.0\",\n",
    "    \"langchain-openai>=0.2.0\",\n",
    "    \"langchain-chroma>=0.2.0\",\n",
    "    \"chromadb>=0.5.0\",\n",
    "    \"atlassian-python-api>=3.41.0\",\n",
    "    \"beautifulsoup4>=4.12.0\",\n",
    "    \"lxml>=5.0.0\",\n",
    "    \"gradio>=4.0.0\",\n",
    "    \"python-dotenv>=1.0.0\",\n",
    "]\n",
    "\n",
    "print(\"ðŸ“¦ Installing dependencies...\")\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        install(pkg)\n",
    "        print(f\"  âœ… {pkg}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Failed to install {pkg}: {e}\")\n",
    "\n",
    "print(\"\\nâœ… All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2b. Import all modules\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import textwrap\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- LangChain Core ---\n",
    "from langchain_community.document_loaders import ConfluenceLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- Gradio ---\n",
    "import gradio as gr\n",
    "\n",
    "# --- Utilities ---\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"âœ… All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. OpenAI API Setup\n",
    "\n",
    "The API key is loaded using the following priority:\n",
    "1. **Google Colab** `userdata.get('OPENAI_API_KEY')` (if running in Colab)\n",
    "2. **Environment variable** `OPENAI_API_KEY`\n",
    "3. **`.env` file** in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. Load API Keys (OpenAI + Confluence)\n",
    "# ============================================================\n",
    "\n",
    "def get_secret(key_name: str, required: bool = True) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Retrieve a secret with fallback chain:\n",
    "      1. Google Colab userdata\n",
    "      2. OS environment variable\n",
    "      3. .env file\n",
    "    \"\"\"\n",
    "    value = None\n",
    "\n",
    "    # 1) Try Colab userdata\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        value = userdata.get(key_name)\n",
    "        if value:\n",
    "            print(f\"  ðŸ”‘ {key_name}: loaded from Colab userdata\")\n",
    "            return value\n",
    "    except (ImportError, Exception):\n",
    "        pass\n",
    "\n",
    "    # 2) Try OS environment\n",
    "    value = os.environ.get(key_name)\n",
    "    if value:\n",
    "        print(f\"  ðŸ”‘ {key_name}: loaded from environment variable\")\n",
    "        return value\n",
    "\n",
    "    # 3) Try .env file\n",
    "    load_dotenv()\n",
    "    value = os.environ.get(key_name)\n",
    "    if value:\n",
    "        print(f\"  ðŸ”‘ {key_name}: loaded from .env file\")\n",
    "        return value\n",
    "\n",
    "    if required:\n",
    "        raise ValueError(\n",
    "            f\"âŒ {key_name} not found! Set it via:\\n\"\n",
    "            f\"   - Colab: Sidebar â†’ ðŸ”‘ Secrets â†’ Add '{key_name}'\\n\"\n",
    "            f\"   - Local: export {key_name}='your-key' or add to .env file\"\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"Loading secrets...\\n\")\n",
    "\n",
    "# --- OpenAI ---\n",
    "OPENAI_API_KEY = get_secret(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# --- Confluence ---\n",
    "CONFLUENCE_URL = get_secret(\"CONFLUENCE_URL\")\n",
    "CONFLUENCE_USERNAME = get_secret(\"CONFLUENCE_USERNAME\", required=False)\n",
    "CONFLUENCE_API_KEY = get_secret(\"CONFLUENCE_API_KEY\", required=False)\n",
    "CONFLUENCE_SPACE_KEY = get_secret(\"CONFLUENCE_SPACE_KEY\")\n",
    "\n",
    "# Optional: Personal Access Token (for Confluence Server/Data Center)\n",
    "CONFLUENCE_PAT = get_secret(\"CONFLUENCE_PAT\", required=False)\n",
    "\n",
    "# --- Validate OpenAI key ---\n",
    "try:\n",
    "    test_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=5)\n",
    "    test_llm.invoke(\"test\")\n",
    "    print(\"\\nâœ… OpenAI API key is valid and working!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ OpenAI API key validation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Load Confluence Pages\n",
    "\n",
    "We use `ConfluenceLoader` from `langchain_community.document_loaders` to load pages from your Confluence space. This loader:\n",
    "\n",
    "- Connects to Confluence via REST API\n",
    "- Recursively loads **all pages including sub-pages** in the specified space\n",
    "- Extracts text content and preserves metadata (page title, URL, page ID, etc.)\n",
    "- Handles nested page hierarchies automatically\n",
    "\n",
    "### Authentication Methods Supported\n",
    "1. **Confluence Cloud** (Atlassian): `username` + `api_key` (API token)\n",
    "2. **Confluence Server/Data Center**: `token` (Personal Access Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. Load Confluence Pages (including sub-pages)\n",
    "# ============================================================\n",
    "\n",
    "def load_confluence_documents(\n",
    "    url: str,\n",
    "    space_key: str,\n",
    "    username: Optional[str] = None,\n",
    "    api_key: Optional[str] = None,\n",
    "    token: Optional[str] = None,\n",
    "    max_pages: int = 250,\n",
    "    include_attachments: bool = False,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load all pages (and sub-pages) from a Confluence space.\n",
    "\n",
    "    Args:\n",
    "        url: Confluence base URL (e.g., https://mycompany.atlassian.net/wiki)\n",
    "        space_key: The Confluence space key (e.g., \"HR\", \"ENG\", \"KB\")\n",
    "        username: Email for Confluence Cloud auth\n",
    "        api_key: API token for Confluence Cloud auth\n",
    "        token: Personal Access Token for Confluence Server/DC\n",
    "        max_pages: Maximum number of pages to load\n",
    "        include_attachments: Whether to load file attachments\n",
    "\n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”„ Connecting to Confluence: {url}\")\n",
    "    print(f\"   Space: {space_key}\")\n",
    "    print(f\"   Max pages: {max_pages}\")\n",
    "    print(f\"   Include attachments: {include_attachments}\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        # Build loader kwargs based on authentication method\n",
    "        loader_kwargs = {\n",
    "            \"url\": url,\n",
    "            \"space_key\": space_key,\n",
    "            \"include_attachments\": include_attachments,\n",
    "            \"limit\": 50,  # Pages per API request (pagination batch size)\n",
    "            \"max_pages\": max_pages,\n",
    "            \"keep_markdown_format\": True,  # Preserve formatting\n",
    "            \"content_format\": \"storage\",   # Get full HTML storage format\n",
    "        }\n",
    "\n",
    "        # Authentication: Cloud (username + api_key) vs Server (PAT)\n",
    "        if token:\n",
    "            loader_kwargs[\"token\"] = token\n",
    "            print(\"   Auth: Personal Access Token (Server/DC)\")\n",
    "        elif username and api_key:\n",
    "            loader_kwargs[\"username\"] = username\n",
    "            loader_kwargs[\"api_key\"] = api_key\n",
    "            print(\"   Auth: Username + API Token (Cloud)\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"âŒ No valid Confluence credentials provided!\\n\"\n",
    "                \"   Cloud: Set CONFLUENCE_USERNAME and CONFLUENCE_API_KEY\\n\"\n",
    "                \"   Server/DC: Set CONFLUENCE_PAT\"\n",
    "            )\n",
    "\n",
    "        # Initialize and load\n",
    "        loader = ConfluenceLoader(**loader_kwargs)\n",
    "        documents = loader.load()\n",
    "\n",
    "        if not documents:\n",
    "            raise ValueError(\n",
    "                f\"âŒ No documents loaded from space '{space_key}'.\\n\"\n",
    "                f\"   Check that the space key is correct and you have read access.\"\n",
    "            )\n",
    "\n",
    "        # Enrich metadata\n",
    "        for i, doc in enumerate(documents):\n",
    "            doc.metadata[\"chunk_index\"] = i\n",
    "            doc.metadata[\"source_type\"] = \"confluence\"\n",
    "            # Ensure page_title is available\n",
    "            if \"title\" in doc.metadata:\n",
    "                doc.metadata[\"page_title\"] = doc.metadata[\"title\"]\n",
    "            elif \"source\" in doc.metadata:\n",
    "                doc.metadata[\"page_title\"] = doc.metadata[\"source\"]\n",
    "            else:\n",
    "                doc.metadata[\"page_title\"] = f\"Page {i+1}\"\n",
    "\n",
    "        print(f\"\\nâœ… Successfully loaded {len(documents)} Confluence pages!\")\n",
    "        print(f\"\\nðŸ“„ Pages loaded:\")\n",
    "        for i, doc in enumerate(documents[:20]):  # Show first 20\n",
    "            title = doc.metadata.get(\"page_title\", \"Untitled\")\n",
    "            content_len = len(doc.page_content)\n",
    "            print(f\"   {i+1:3d}. {title} ({content_len:,} chars)\")\n",
    "        if len(documents) > 20:\n",
    "            print(f\"   ... and {len(documents) - 20} more pages\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error loading Confluence pages: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# --- Load documents ---\n",
    "raw_documents = load_confluence_documents(\n",
    "    url=CONFLUENCE_URL,\n",
    "    space_key=CONFLUENCE_SPACE_KEY,\n",
    "    username=CONFLUENCE_USERNAME,\n",
    "    api_key=CONFLUENCE_API_KEY,\n",
    "    token=CONFLUENCE_PAT,\n",
    "    max_pages=250,\n",
    "    include_attachments=False,\n",
    ")\n",
    "\n",
    "# Show sample content\n",
    "if raw_documents:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ“‹ Sample content from first page:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Title: {raw_documents[0].metadata.get('page_title', 'N/A')}\")\n",
    "    print(f\"Content preview (first 500 chars):\\n\")\n",
    "    print(raw_documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Chunking / Text Splitting\n",
    "\n",
    "### Why chunk?\n",
    "Confluence pages can be very long. Embedding entire pages reduces retrieval precision â€” the embedding of a 10,000-character page is a blurry average of many topics. Chunking creates focused, semantically coherent pieces that match specific queries better.\n",
    "\n",
    "### Parameter Choices\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|---|---|---|\n",
    "| `chunk_size` | **1000** | Balances semantic completeness with retrieval precision. Large enough to capture a full paragraph/concept, small enough for targeted retrieval. |\n",
    "| `chunk_overlap` | **200** | 20% overlap ensures context continuity â€” sentences at chunk boundaries aren't lost. Prevents splitting mid-thought. |\n",
    "| `separators` | `[\"\\n\\n\", \"\\n\", \". \", \" \"]` | Splits on natural boundaries: paragraphs â†’ lines â†’ sentences â†’ words. Preserves document structure. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. Text Splitting\n",
    "# ============================================================\n",
    "\n",
    "CHUNK_SIZE = 1000       # Characters per chunk\n",
    "CHUNK_OVERLAP = 200     # Overlap between consecutive chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"],\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Filter out empty or near-empty documents before splitting\n",
    "valid_documents = [\n",
    "    doc for doc in raw_documents\n",
    "    if doc.page_content and len(doc.page_content.strip()) > 50\n",
    "]\n",
    "print(f\"ðŸ“„ Valid documents (non-empty, >50 chars): {len(valid_documents)} / {len(raw_documents)}\")\n",
    "\n",
    "# Split into chunks\n",
    "chunks = text_splitter.split_documents(valid_documents)\n",
    "\n",
    "# Add chunk-level metadata for citation\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.metadata[\"chunk_id\"] = f\"chunk_{i:04d}\"\n",
    "    chunk.metadata[\"chunk_char_count\"] = len(chunk.page_content)\n",
    "\n",
    "print(f\"\\nâœ… Text splitting complete!\")\n",
    "print(f\"   ðŸ“Š Total chunks created: {len(chunks)}\")\n",
    "print(f\"   ðŸ“ Chunk size: {CHUNK_SIZE} chars\")\n",
    "print(f\"   ðŸ”— Chunk overlap: {CHUNK_OVERLAP} chars\")\n",
    "\n",
    "# Distribution stats\n",
    "chunk_lengths = [len(c.page_content) for c in chunks]\n",
    "print(f\"\\n   ðŸ“ˆ Chunk length statistics:\")\n",
    "print(f\"      Min: {min(chunk_lengths):,} chars\")\n",
    "print(f\"      Max: {max(chunk_lengths):,} chars\")\n",
    "print(f\"      Avg: {sum(chunk_lengths)/len(chunk_lengths):,.0f} chars\")\n",
    "\n",
    "# Show sample chunk\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸ“‹ Sample chunk (chunk_0000):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Page: {chunks[0].metadata.get('page_title', 'N/A')}\")\n",
    "print(f\"Chunk ID: {chunks[0].metadata.get('chunk_id', 'N/A')}\")\n",
    "print(f\"Content:\\n{chunks[0].page_content[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Embeddings + ChromaDB Vector Store\n",
    "\n",
    "We use OpenAI's `text-embedding-ada-002` model via `OpenAIEmbeddings` to convert text chunks into dense vectors, then store them in an **in-memory ChromaDB** instance for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Create Embeddings & ChromaDB Vector Store\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ”„ Creating embeddings and building vector store...\")\n",
    "print(f\"   Model: text-embedding-ada-002\")\n",
    "print(f\"   Chunks to embed: {len(chunks)}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Initialize embeddings\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "    )\n",
    "\n",
    "    # Build ChromaDB vector store (in-memory)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"confluence_rag\",\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},  # Cosine similarity\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Vector store created successfully!\")\n",
    "    print(f\"   ðŸ“Š Total vectors stored: {vectorstore._collection.count()}\")\n",
    "    print(f\"   ðŸ’¾ Storage: In-memory (ChromaDB)\")\n",
    "    print(f\"   ðŸ“ Distance metric: Cosine similarity\")\n",
    "\n",
    "    # Quick test retrieval\n",
    "    test_results = vectorstore.similarity_search(\"test\", k=1)\n",
    "    if test_results:\n",
    "        print(f\"\\n   ðŸ§ª Test retrieval: OK (retrieved {len(test_results)} result)\")\n",
    "    else:\n",
    "        print(f\"\\n   âš ï¸  Test retrieval returned no results.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error creating vector store: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Retriever + RAG QA Chain (GPT-3.5 Turbo)\n",
    "\n",
    "We build the RAG chain using **modern LangChain LCEL (LangChain Expression Language)** patterns â€” no deprecated `ConversationalRetrievalChain` or `RetrievalQA`.\n",
    "\n",
    "### Chain Architecture\n",
    "```\n",
    "User Input â†’ Format chat history â†’ Retrieve relevant chunks (top_k)\n",
    "     â†“\n",
    "Strict Prompt Template (system + context + history + question)\n",
    "     â†“\n",
    "GPT-3.5 Turbo (temperature=0.1) â†’ Grounded answer with citations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Prompt Template (Strict Grounding + Citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7 & 8. LLM, Prompt Template, and RAG Chain\n",
    "# ============================================================\n",
    "\n",
    "# --- Initialize LLM ---\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,             # Low temperature for factual accuracy\n",
    "    max_tokens=1024,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    ")\n",
    "print(\"âœ… LLM initialized: gpt-3.5-turbo (temperature=0.1)\")\n",
    "\n",
    "\n",
    "# --- Strict Prompt Template ---\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful, accurate, and concise assistant that answers questions ONLY based on the provided Confluence page context.\n",
    "\n",
    "## STRICT RULES:\n",
    "1. **ONLY answer from the provided context.** Do NOT use any prior knowledge or make assumptions.\n",
    "2. **If the answer is not in the context, say exactly:** \"I don't know based on the provided document.\"\n",
    "3. **Always cite your sources** using the page title and chunk ID from the metadata.\n",
    "4. **Keep answers concise** â€” aim for 2-5 sentences unless a detailed answer is clearly needed.\n",
    "5. **Be compliance-friendly** â€” do not speculate, do not provide personal opinions.\n",
    "6. **Format citations** at the end of your answer like:\n",
    "   ðŸ“– Sources:\n",
    "   - [Page Title] (Chunk: chunk_XXXX)\n",
    "\n",
    "## CONTEXT FROM CONFLUENCE PAGES:\n",
    "{context}\n",
    "\n",
    "## CONVERSATION HISTORY:\n",
    "{chat_history}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "print(\"âœ… Prompt template created (strict grounding + citations)\")\n",
    "\n",
    "\n",
    "# --- Helper functions ---\n",
    "def format_docs_with_metadata(docs: List[Document]) -> str:\n",
    "    \"\"\"Format retrieved documents with metadata for the prompt context.\"\"\"\n",
    "    if not docs:\n",
    "        return \"No relevant documents found.\"\n",
    "\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        page_title = doc.metadata.get(\"page_title\", \"Unknown Page\")\n",
    "        chunk_id = doc.metadata.get(\"chunk_id\", f\"chunk_{i}\")\n",
    "        source_url = doc.metadata.get(\"source\", \"\")\n",
    "\n",
    "        header = f\"--- Source {i+1}: [{page_title}] (Chunk: {chunk_id}) ---\"\n",
    "        if source_url:\n",
    "            header += f\"\\nURL: {source_url}\"\n",
    "\n",
    "        formatted.append(f\"{header}\\n{doc.page_content}\\n\")\n",
    "\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def format_chat_history(history: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"Format conversation history for the prompt.\"\"\"\n",
    "    if not history:\n",
    "        return \"No previous conversation.\"\n",
    "\n",
    "    formatted = []\n",
    "    for human, ai in history[-5:]:  # Keep last 5 exchanges to manage context window\n",
    "        formatted.append(f\"Human: {human}\")\n",
    "        formatted.append(f\"Assistant: {ai}\")\n",
    "\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def extract_source_citations(docs: List[Document]) -> str:\n",
    "    \"\"\"Extract unique source citations from retrieved documents.\"\"\"\n",
    "    seen = set()\n",
    "    citations = []\n",
    "\n",
    "    for doc in docs:\n",
    "        page_title = doc.metadata.get(\"page_title\", \"Unknown\")\n",
    "        chunk_id = doc.metadata.get(\"chunk_id\", \"N/A\")\n",
    "        source_url = doc.metadata.get(\"source\", \"\")\n",
    "        excerpt = doc.page_content[:100].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "        key = f\"{page_title}_{chunk_id}\"\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            citation = f\"- **{page_title}** (Chunk: {chunk_id})\"\n",
    "            if source_url:\n",
    "                citation += f\" | [Link]({source_url})\"\n",
    "            citation += f\"\\n  _Excerpt: \\\"{excerpt}...\\\"_\"\n",
    "            citations.append(citation)\n",
    "\n",
    "    return \"\\n\".join(citations) if citations else \"No sources available.\"\n",
    "\n",
    "\n",
    "# --- Build the RAG chain (LCEL) ---\n",
    "def create_rag_chain(top_k: int = 4):\n",
    "    \"\"\"\n",
    "    Create a RAG chain using LCEL with configurable top_k.\n",
    "    Returns: (chain, retriever)\n",
    "    \"\"\"\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": top_k},\n",
    "    )\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": lambda x: format_docs_with_metadata(\n",
    "                retriever.invoke(x[\"question\"])\n",
    "            ),\n",
    "            \"chat_history\": lambda x: format_chat_history(x.get(\"chat_history\", [])),\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain, retriever\n",
    "\n",
    "\n",
    "# Create default chain\n",
    "rag_chain, retriever = create_rag_chain(top_k=4)\n",
    "print(\"âœ… RAG chain built with LCEL (top_k=4)\")\n",
    "\n",
    "\n",
    "# --- Query function with full error handling ---\n",
    "def query_rag(\n",
    "    question: str,\n",
    "    chat_history: List[Tuple[str, str]] = None,\n",
    "    top_k: int = 4,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Query the RAG chain with error handling and citation extraction.\n",
    "\n",
    "    Returns:\n",
    "        dict with 'answer', 'sources', 'retrieved_docs'\n",
    "    \"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    if not question or not question.strip():\n",
    "        return {\n",
    "            \"answer\": \"Please enter a question.\",\n",
    "            \"sources\": \"\",\n",
    "            \"retrieved_docs\": [],\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # Recreate chain if top_k changed\n",
    "        chain, ret = create_rag_chain(top_k=top_k)\n",
    "\n",
    "        # Retrieve documents for citation\n",
    "        retrieved_docs = ret.invoke(question)\n",
    "\n",
    "        # Check for empty retrieval\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"I don't know based on the provided document. No relevant content was found in the Confluence pages for your query.\",\n",
    "                \"sources\": \"No matching documents found.\",\n",
    "                \"retrieved_docs\": [],\n",
    "            }\n",
    "\n",
    "        # Run chain\n",
    "        answer = chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"chat_history\": chat_history,\n",
    "        })\n",
    "\n",
    "        # Extract citations\n",
    "        sources = extract_source_citations(retrieved_docs)\n",
    "\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources,\n",
    "            \"retrieved_docs\": retrieved_docs,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"rate_limit\" in error_msg.lower() or \"429\" in error_msg:\n",
    "            return {\n",
    "                \"answer\": \"âš ï¸ OpenAI rate limit reached. Please wait a moment and try again.\",\n",
    "                \"sources\": \"\",\n",
    "                \"retrieved_docs\": [],\n",
    "            }\n",
    "        elif \"authentication\" in error_msg.lower() or \"401\" in error_msg:\n",
    "            return {\n",
    "                \"answer\": \"âŒ OpenAI API authentication error. Please check your API key.\",\n",
    "                \"sources\": \"\",\n",
    "                \"retrieved_docs\": [],\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"answer\": f\"âŒ An error occurred: {error_msg}\",\n",
    "                \"sources\": \"\",\n",
    "                \"retrieved_docs\": [],\n",
    "            }\n",
    "\n",
    "\n",
    "print(\"\\nâœ… RAG query function ready with full error handling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Quick test (before building UI)\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ§ª Running a quick test query...\\n\")\n",
    "\n",
    "test_result = query_rag(\"What are the main topics covered in these pages?\", top_k=3)\n",
    "\n",
    "print(f\"Question: What are the main topics covered in these pages?\")\n",
    "print(f\"\\nAnswer:\\n{test_result['answer']}\")\n",
    "print(f\"\\nðŸ“– Sources:\\n{test_result['sources']}\")\n",
    "print(f\"\\nâœ… Test complete! Retrieved {len(test_result['retrieved_docs'])} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Gradio Chat UI\n",
    "\n",
    "The Gradio interface provides:\n",
    "- **Chat-style conversation** with persistent session memory\n",
    "- **Clear Chat** button to reset conversation\n",
    "- **top_k slider** to control how many chunks are retrieved (1â€“10)\n",
    "- **Sources panel** showing citations with page titles, chunk IDs, and excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. Gradio Chat UI\n",
    "# ============================================================\n",
    "\n",
    "def respond(\n",
    "    message: str,\n",
    "    chat_history: List[List[str]],\n",
    "    top_k: int,\n",
    ") -> Tuple[str, List[List[str]], str]:\n",
    "    \"\"\"\n",
    "    Process user message through RAG pipeline and return response.\n",
    "\n",
    "    Args:\n",
    "        message: User's question\n",
    "        chat_history: List of [user_msg, bot_msg] pairs (Gradio format)\n",
    "        top_k: Number of chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        (bot_response, updated_history, sources_text)\n",
    "    \"\"\"\n",
    "    if not message.strip():\n",
    "        return \"\", chat_history, \"\"\n",
    "\n",
    "    # Convert Gradio history format to our tuple format\n",
    "    history_tuples = [(h[0], h[1]) for h in chat_history if len(h) == 2 and h[1]]\n",
    "\n",
    "    # Query RAG\n",
    "    result = query_rag(\n",
    "        question=message,\n",
    "        chat_history=history_tuples,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    answer = result[\"answer\"]\n",
    "    sources = result[\"sources\"]\n",
    "\n",
    "    # Update history\n",
    "    chat_history.append([message, answer])\n",
    "\n",
    "    # Format sources display\n",
    "    sources_display = f\"### ðŸ“– Sources (top_{top_k} retrieval)\\n\\n{sources}\" if sources else \"No sources.\"\n",
    "\n",
    "    return \"\", chat_history, sources_display\n",
    "\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Clear conversation history and sources.\"\"\"\n",
    "    return [], \"\", \"\"\n",
    "\n",
    "\n",
    "# --- Build Gradio Interface ---\n",
    "with gr.Blocks(\n",
    "    title=\"ðŸ“š Confluence RAG Chatbot\",\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\"\"\"\n",
    "    .source-box { max-height: 300px; overflow-y: auto; }\n",
    "    footer { display: none !important; }\n",
    "    \"\"\"\n",
    ") as demo:\n",
    "\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ðŸ“š Confluence RAG Chatbot\n",
    "        ### Ask questions about your Confluence pages â€” answers are grounded in your documents with citations.\n",
    "\n",
    "        **Features:** Conversational memory â€¢ Adjustable retrieval depth â€¢ Source citations\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"Conversation\",\n",
    "                height=500,\n",
    "                show_copy_button=True,\n",
    "                bubble_full_width=False,\n",
    "                avatar_images=(None, \"https://em-content.zobj.net/source/twitter/376/robot_1f916.png\"),\n",
    "            )\n",
    "\n",
    "            with gr.Row():\n",
    "                msg_input = gr.Textbox(\n",
    "                    placeholder=\"Ask a question about your Confluence pages...\",\n",
    "                    label=\"Your Question\",\n",
    "                    scale=4,\n",
    "                    lines=1,\n",
    "                    max_lines=3,\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send ðŸ“¤\", variant=\"primary\", scale=1)\n",
    "\n",
    "            with gr.Row():\n",
    "                clear_btn = gr.Button(\"ðŸ—‘ï¸ Clear Chat\", variant=\"secondary\")\n",
    "                top_k_slider = gr.Slider(\n",
    "                    minimum=1,\n",
    "                    maximum=10,\n",
    "                    value=4,\n",
    "                    step=1,\n",
    "                    label=\"Retrieval Depth (top_k)\",\n",
    "                    info=\"Number of document chunks to retrieve per query\",\n",
    "                )\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            sources_display = gr.Markdown(\n",
    "                value=\"### ðŸ“– Sources\\n\\n_Ask a question to see source citations here._\",\n",
    "                label=\"Sources & Citations\",\n",
    "                elem_classes=[\"source-box\"],\n",
    "            )\n",
    "\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                ---\n",
    "                ### â„¹ï¸ How to Use\n",
    "                1. Type your question in the text box\n",
    "                2. Adjust **top_k** slider for more/fewer source chunks\n",
    "                3. View citations in the **Sources** panel\n",
    "                4. Click **Clear Chat** to start a new conversation\n",
    "\n",
    "                ### ðŸ›¡ï¸ Grounding Policy\n",
    "                - Answers come **only** from loaded Confluence pages\n",
    "                - If info isn't found, the bot will say so\n",
    "                - Citations include page title and chunk ID\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "    # --- Event handlers ---\n",
    "    send_btn.click(\n",
    "        fn=respond,\n",
    "        inputs=[msg_input, chatbot, top_k_slider],\n",
    "        outputs=[msg_input, chatbot, sources_display],\n",
    "    )\n",
    "\n",
    "    msg_input.submit(\n",
    "        fn=respond,\n",
    "        inputs=[msg_input, chatbot, top_k_slider],\n",
    "        outputs=[msg_input, chatbot, sources_display],\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn=clear_chat,\n",
    "        inputs=[],\n",
    "        outputs=[chatbot, sources_display, msg_input],\n",
    "    )\n",
    "\n",
    "print(\"âœ… Gradio UI built successfully!\")\n",
    "print(\"ðŸš€ Launching...\")\n",
    "\n",
    "# Launch the UI\n",
    "demo.launch(\n",
    "    share=False,        # Set True for public Colab link\n",
    "    debug=False,\n",
    "    server_name=\"0.0.0.0\",  # Accessible from any interface\n",
    "    server_port=7860,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Demo + Testing\n",
    "\n",
    "Below are test queries you can run programmatically to validate the pipeline (without the Gradio UI).\n",
    "\n",
    "> **Note:** These demonstrate expected behavior â€” actual answers depend on YOUR Confluence content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. Demo / Testing (programmatic, no UI needed)\n",
    "# ============================================================\n",
    "\n",
    "# Stop the Gradio server first if running\n",
    "try:\n",
    "    demo.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ§ª DEMO: RAG Pipeline Testing\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Test queries ---\n",
    "test_questions = [\n",
    "    \"What are the main topics covered in these Confluence pages?\",\n",
    "    \"Summarize the key policies or guidelines mentioned.\",\n",
    "    \"What is the process described for onboarding?\",      # May say \"I don't know\" if not in pages\n",
    "    \"Tell me about quantum computing advancements.\",       # Should say \"I don't know\" â€” off-topic\n",
    "]\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(f\"ðŸ”¹ Test {i}: {question}\")\n",
    "    print(f\"{'â”€'*60}\")\n",
    "\n",
    "    result = query_rag(question, chat_history=chat_history, top_k=4)\n",
    "\n",
    "    print(f\"\\nðŸ’¬ Answer:\\n{result['answer']}\")\n",
    "    print(f\"\\nðŸ“– Sources:\\n{result['sources']}\")\n",
    "    print(f\"\\nðŸ“Š Chunks retrieved: {len(result['retrieved_docs'])}\")\n",
    "\n",
    "    # Add to history for conversational continuity\n",
    "    chat_history.append((question, result['answer']))\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… Demo complete! {len(test_questions)} queries tested.\")\n",
    "print(f\"   Conversation history: {len(chat_history)} exchanges\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# --- Expected behavior ---\n",
    "print(\"\"\"\n",
    "ðŸ“‹ EXPECTED BEHAVIOR:\n",
    "\n",
    "1. Questions about Confluence content â†’ Grounded answer + citations\n",
    "   Example: \"What is the leave policy?\" â†’ Answer from HR pages + page title + chunk ID\n",
    "\n",
    "2. Off-topic questions â†’ \"I don't know based on the provided document.\"\n",
    "   Example: \"Tell me about quantum computing\" â†’ Polite refusal\n",
    "\n",
    "3. Follow-up questions â†’ Uses conversation history for context\n",
    "   Example: \"Can you elaborate on that?\" â†’ References previous answer's topic\n",
    "\n",
    "4. Empty retrieval â†’ Clear message that no relevant content was found\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Troubleshooting / Common Errors\n",
    "\n",
    "### ðŸ”‘ API Key Issues\n",
    "\n",
    "| Error | Cause | Fix |\n",
    "|---|---|---|\n",
    "| `OPENAI_API_KEY not found` | Key not set | **Colab:** Add to Secrets sidebar. **Local:** `export OPENAI_API_KEY='sk-...'` or add to `.env` |\n",
    "| `AuthenticationError` | Invalid or expired key | Generate a new key at [platform.openai.com](https://platform.openai.com/api-keys) |\n",
    "| `RateLimitError` | Too many requests | Wait 60 seconds and retry. Consider upgrading your OpenAI plan. |\n",
    "\n",
    "### ðŸ“„ Confluence Loading Issues\n",
    "\n",
    "| Error | Cause | Fix |\n",
    "|---|---|---|\n",
    "| `No documents loaded` | Wrong space key or no access | Verify `CONFLUENCE_SPACE_KEY` and that your credentials have read access |\n",
    "| `401 Unauthorized` | Bad credentials | **Cloud:** Use email + API token (not password). **Server:** Use Personal Access Token |\n",
    "| `ConnectionError` | Wrong URL | Ensure URL format: `https://your-domain.atlassian.net/wiki` |\n",
    "| `Empty pages` | Pages have no text content | Check if pages use macros/embedded content that the loader can't extract |\n",
    "\n",
    "### ðŸ”§ Module/Import Errors\n",
    "\n",
    "| Error | Fix |\n",
    "|---|---|\n",
    "| `ModuleNotFoundError: langchain_community` | Re-run the install cell (Section 2) |\n",
    "| `ImportError: cannot import name 'ConfluenceLoader'` | `pip install -q atlassian-python-api lxml beautifulsoup4` |\n",
    "| `ModuleNotFoundError: langchain_chroma` | `pip install -q langchain-chroma` |\n",
    "\n",
    "### ðŸ’¡ Common Runtime Issues\n",
    "\n",
    "| Issue | Fix |\n",
    "|---|---|\n",
    "| Gradio port in use | Change `server_port` in `demo.launch()` or kill process: `lsof -i :7860` |\n",
    "| Slow embeddings | Normal for first run with many chunks. ChromaDB caches after that. |\n",
    "| `ChromaDB` warnings | Safely ignorable. Set `warnings.filterwarnings('ignore')` |\n",
    "| Chat memory too long | The prompt limits history to last 5 exchanges to manage context window |\n",
    "\n",
    "### ðŸ”„ Confluence Authentication Guide\n",
    "\n",
    "**For Confluence Cloud (Atlassian):**\n",
    "1. Go to [id.atlassian.com/manage-profile/security/api-tokens](https://id.atlassian.com/manage-profile/security/api-tokens)\n",
    "2. Create an API token\n",
    "3. Set `CONFLUENCE_USERNAME` = your email, `CONFLUENCE_API_KEY` = the token\n",
    "\n",
    "**For Confluence Server/Data Center:**\n",
    "1. Go to Profile â†’ Personal Access Tokens\n",
    "2. Create a new token with read permissions\n",
    "3. Set `CONFLUENCE_PAT` = the token\n",
    "\n",
    "### ðŸ“§ Still stuck?\n",
    "Check LangChain docs: [python.langchain.com/docs/integrations/document_loaders/confluence](https://python.langchain.com/docs/integrations/document_loaders/confluence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
